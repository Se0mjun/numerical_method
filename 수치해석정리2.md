# 행렬과 노름: 현대 계산 과학의 핵심 도구

## 1.1 행렬 곱셈 (matrix multiplication)

행렬 곱셈은 선형대수학 및 수치선형대수에서 핵심적인 연산으로, 다양한 공학 및 과학 분야에서 시스템의 변환, 회전, 스케일링 등을 모델링하는 데 사용된다. 또한, 행렬 곱셈은 데이터 분석, 기계학습 알고리즘의 구현, 특히 신경망의 전파 및 역전파 과정에서 중요한 역할을 수행하며, 계산 문제를 해결하는 데 있어서 핵심적인 도구라고 볼 수 있다.

> **현대적 관점**: 행렬 곱셈은 단순한 수학적 연산을 넘어 정보 변환의 관점에서 이해할 수 있다. 딥러닝에서 각 레이어의 가중치 행렬은 입력 데이터의 특징 공간(feature space)을 새로운 표현 공간으로 변환하는 역할을 한다. 이러한 관점에서 행렬 곱셈은 데이터의 의미있는 패턴을 추출하고 증폭하는 정보 처리 메커니즘으로 볼 수 있다.

특히, 최근에 구글 딥마인드에서 발표한 알파텐서(AlphaTensor)는 고차원 데이터를 다루는 복잡한 계산 과정에서 다차원 행렬(텐서) 간의 연산을 효율적으로 줄이는 방법을 제시하여 대용량 AI 모델에서 훈련 시간을 크게 단축하는 효과를 만들어냈다. 

> **알고리즘 혁신**: AlphaTensor가 발견한 행렬 곱셈 알고리즘은 1969년 Volker Strassen이 발견한 알고리즘 이후 약 50년 만의 주요 혁신이다. Strassen의 알고리즘은 $n \times n$ 행렬 곱셈의 복잡도를 $O(n^3)$에서 $O(n^{2.807})$로 줄였고, 현재까지 알려진 가장 빠른 알고리즘의 이론적 복잡도는 약 $O(n^{2.3728639})$이다. 이는 행렬 곱셈 최적화가 계산 과학에서 얼마나 중요한지 보여준다.

두 행렬의 곱셈을 설명하기 전에 먼저 행렬과 벡터의 곱셈에 대하여 알아보자. 선형 대수학에서 배운 것처럼 일반적인 두 행렬의 곱셈처럼 계산 가능하지만 다음 방법을 이해하는 것이 굉장히 중요하다.

**정의 1.1.** $m \times n$ 행렬 $A$의 $j$ 번째 열(column)을 $a_j$ 라고 하자. 여기서 $a_j$ 는 $m$차원 벡터이다. 그러면 행렬 $A$와 열벡터 $x$의 곱셈은 다음과 같이 다시 쓸 수 있다.

$$Ax = \sum_{j=1}^{n} x_j a_j$$

구체적으로, $m \times n$ 행렬 $A$와 $n$-벡터 $x$와의 곱셈은 다음과 같다.

$$Ax = \begin{bmatrix} | & | & | \\ a_1 & a_2 & \cdots & a_n \\ | & | & | \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = x_1 \begin{bmatrix} | \\ a_1 \\ | \end{bmatrix} + x_2 \begin{bmatrix} | \\ a_2 \\ | \end{bmatrix} + \cdots + x_n \begin{bmatrix} | \\ a_n \\ | \end{bmatrix}.$$

즉, $Ax$는 열 벡터(column vector) $a_1, \ldots, a_n$의 선형 결합(linear combination)으로 표현된다.

이제 두 행렬 $A$와 $B$의 곱셈을 고려해보자. $A$가 $m \times n$ 행렬이고, $B$가 $n \times p$ 행렬일 때, 그들의 곱 $AB$는 $m \times p$ 행렬 $C$를 생성한다. $C$의 각 원소 $ij$는 다음과 같이 세 가지 방법으로 계산이 가능하다.

(I) 표준 방법(standard method)  
(II) 행렬-벡터 곱셈 이용  
(III) 외적(outer product) 이용

### 1.1.1 행렬 곱셈 I: 표준 방법(standard method)

$C$의 각 원소 $c_{ij}$를 다음과 같이 계산한다.

$$c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}$$

여기서 $a_{ik}$는 $A$의 $i$번째 행과 $k$번째 열의 원소이고, $b_{kj}$는 $B$의 $k$번째 행과 $j$ 번째 열의 원소이다. 이 과정은 모든 $i, j$에 대하여 반복된다.

**예제 1.1**
$$\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \begin{bmatrix} b_{11} & b_{12} & b_{13} \\ b_{21} & b_{22} & b_{23} \end{bmatrix} = \begin{bmatrix} a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} & a_{11}b_{13} + a_{12}b_{23} \\ a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22} & a_{21}b_{13} + a_{22}b_{23} \\ a_{31}b_{11} + a_{32}b_{21} & a_{31}b_{12} + a_{32}b_{22} & a_{31}b_{13} + a_{32}b_{23} \end{bmatrix}$$

### 1.1.2 행렬 곱셈 II: 행렬-벡터 곱 이용

$n \times p$ 행렬 $B$의 $j$ 번째 열을 $b_j$ 라고 하자. 여기서 $b_j$ 는 $n$-벡터이다. 그러면 행렬 $C$의 $j$ 번째 열 $c_j$ 는 $Ab_j$ 로 계산 가능하다. 그러므로,

$$C = A \begin{bmatrix} | & | & | \\ b_1 & b_2 & \cdots & b_p \\ | & | & | \end{bmatrix} = \begin{bmatrix} | & | & | \\ Ab_1 & Ab_2 & \cdots & Ab_p \\ | & | & | \end{bmatrix}.$$

**예제 1.2**
$$\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \begin{bmatrix} b_{11} & b_{12} & b_{13} \\ b_{21} & b_{22} & b_{23} \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \begin{bmatrix} b_{11} \\ b_{21} \end{bmatrix} & \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \begin{bmatrix} b_{12} \\ b_{22} \end{bmatrix} & \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \begin{bmatrix} b_{13} \\ b_{23} \end{bmatrix} \end{bmatrix}$$

### 1.1.3 행렬 곱셈 III: 외적(outer product) 이용

두 벡터 $u$와 $v$의 내적 $\langle u, v \rangle$는 행렬 곱셈으로 $u^T v$으로 표현 가능하다. 내적은 스칼라인데 반해 외적(outer product)는 행렬로 표현된다.

**정의 1.2.** 두 벡터 $u = (u_1, \ldots, u_m) \in \mathbb{R}^m$ 와 $v = (v_1, \ldots, v_n) \in \mathbb{R}^n$의 외적은 $m \times n$ 행렬로 다음과 같이 정의된다.

$$u \otimes v = uv^T = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_m \end{bmatrix} \begin{bmatrix} v_1 & v_2 & \cdots & v_n \end{bmatrix} = \begin{bmatrix} u_1v_1 & u_1v_2 & \cdots & u_1v_n \\ u_2v_1 & u_2v_2 & \cdots & u_2v_n \\ \vdots & \vdots & \ddots & \vdots \\ u_mv_1 & u_mv_2 & \cdots & u_mv_n \end{bmatrix}$$

여기서 $u \otimes v$는 두 벡터의 외적을 나타내며, 각 요소 $(i, j)$는 $u$의 $i$번째 원소와 $v$의 $j$ 번째 원소의 곱이다. 외적은 계수(rank)가 1인 행렬(rank-one matrix)이며 다음 성질을 가진다.

**기초정리 1.3.** 두 벡터 $u$와 $v$ 외적은 다음을 만족한다.
* $(u \otimes v)^T = (v \otimes u)$
* $(v + w) \otimes u = v \otimes u + w \otimes u$
* $u \otimes (v + w) = u \otimes v + u \otimes w$
* $c(v \otimes u) = (cv) \otimes u = v \otimes (cu)$

여기서 $c$는 스칼라이다.

$m \times n$ 행렬 $A$와 $n \times p$ 행렬 $B$의 곱셈 $C$는 두 행렬의 외적을 통한 곱으로 다음과 같이 계산 가능하다.

$$C = AB = \sum_{i=1}^{n} a_i b_i^T$$

여기서 $a_i$는 $A$의 $i$번째 열 벡터이고, $b_i^T$는 $B$의 $i$번째 행 벡터이다. 결과적으로, 외적으로 생성된 행렬 $C$는 이러한 $n$ 개의 외적의 합으로 구성된다. 예를 들면,

$$\begin{bmatrix} | & | & | \\ a_1 & a_2 & a_3 \\ | & | & | \end{bmatrix} \begin{bmatrix} - b_1^T - \\ - b_2^T - \\ - b_3^T - \end{bmatrix} = \begin{bmatrix} | \\ a_1 \\ | \end{bmatrix} \begin{bmatrix} - b_1^T - \end{bmatrix} + \begin{bmatrix} | \\ a_2 \\ | \end{bmatrix} \begin{bmatrix} - b_2^T - \end{bmatrix} + \begin{bmatrix} | \\ a_3 \\ | \end{bmatrix} \begin{bmatrix} - b_3^T - \end{bmatrix}$$

**예제 1.3**
$$\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \begin{bmatrix} v_{11} & v_{12} & v_{13} & v_{14} \\ v_{21} & v_{22} & v_{23} & v_{24} \end{bmatrix} = \begin{bmatrix} a_{11} \\ a_{21} \\ a_{31} \end{bmatrix} \begin{bmatrix} v_{11} & v_{12} & v_{13} & v_{14} \end{bmatrix} + \begin{bmatrix} a_{12} \\ a_{22} \\ a_{32} \end{bmatrix} \begin{bmatrix} v_{21} & v_{22} & v_{23} & v_{24} \end{bmatrix}$$

## 1.2 행렬의 거듭제곱 (power of matrix)

거듭제곱(power)은 어떤 수나 표현식을 반복하여 곱하는 연산을 말한다. 수 $a$의 $n$ 거듭제곱은 $a$를 $n$번 자기 자신과 곱하는 것을 의미하며, 다음과 같이 표현된다.
$a^n = a \times a \times \cdots \times a$

> **동적 시스템에서의 의미**: 행렬의 거듭제곱은 동적 시스템 이론에서 중요한 의미를 갖는다. 마르코프 체인(Markov chain)에서 상태 전이 행렬 $P$의 $n$번째 거듭제곱 $P^n$은 $n$번의 단계 후 시스템의 상태 분포를 나타낸다. 또한 $\lim_{n \to \infty} P^n$는 시스템의 정상 상태(steady state)를 보여주는데, 이는 네트워크 분석과 확률적 모델링에서 핵심적인 개념이다.

여기서 $a$는 밑(base)이라고 하며, $n$은 지수(exponent)라고 한다.

(1) 자연수 거듭제곱은 다음과 같이 정의한다.
   $$a^n = a \times a \times \cdots \times a \text{ ($n$번)}$$
   여기서 $n$은 자연수이다.

(2) 정수 거듭제곱은 다음과 같이 정의한다.
   $$a^{-n} = (a^{-1})^n$$
   여기서 $a^{-1}$은 $a$의 곱셈에 대한 역원(inverse)이다. 그리고 $a$의 0 거듭제곱은 1로 정의한다. (단, $a \neq 0$)

(3) 유리수 거듭제곱은 다음과 같이 정의한다.
   $$a^{\frac{m}{n}} = (a^{\frac{1}{n}})^m$$
   $a^{\frac{1}{n}}$는 $a$의 $n$제곱근이다. 즉, $x^n = a$를 만족하는 양의 해 $x$이다. 여기서 $m$과 $n$은 정수이며, $n$은 0이 아니다.

(4) 실수 거듭제곱는 다음과 같이 정의한다.
   만약 $r$이 무리수라면, $r$을 근사하는 유리수 수열 $\{r_n\}$이 존재한다. 이 수열에 대해, $a^r$은 다음과 같이 정의된다:
   $$a^r = \lim_{n\to\infty} a^{r_n}$$
   여기서 $a^{r_n}$은 $r_n$이 유리수일 때 이미 정의된 거듭제곱을 사용하여 계산된다. 이 방법으로 $a^r$는 모든 실수 $r$에 대해 잘 정의된다(well-defined).

마찬가지로 행렬 $A$의 거듭제곱도 정의가능하다.

(1) 자연수 거듭제곱은 다음과 같이 정의한다.
   $$A^n = A \times A \times \cdots \times A \text{ ($n$번)}$$
   여기서 $n$은 자연수이다.

(2) 정수 거듭제곱은 다음과 같이 정의한다.
   $$A^{-n} = (A^{-1})^n$$
   여기서 $A^{-1}$은 $A$의 곱셈에 대한 역원(inverse)이다. 단, 역행렬이 존재하는 행렬만 다룰수 있다. 그리고 $A$의 0 거듭제곱은 1로 정의한다. (단, $a \neq 0$)

(3) 유리수 거듭제곱은 다음과 같이 정의한다.
   $$A^{\frac{m}{n}} = (A^{\frac{1}{n}})^m$$
   여기서 $m$과 $n$은 정수이며, $n$은 0이 아니다. $A^{\frac{1}{n}}$는 $A$의 $n$제곱근이다. 즉, $X^n = A$를 만족하는 양의 해 $X$이다. 단, 모든 행렬이 $n$제곱근을 가지는 것은 아니다. 양의 정부호행렬(positive definite matrix)은 $n$제곱근이 항상 존재한다.

(4) 실수 거듭제곱는 다음과 같이 정의한다.
   만약 $r$이 무리수라면, $r$을 근사하는 유리수 수열 $\{r_n\}$이 존재한다. 이 수열에 대해, $A^r$은 다음과 같이 정의된다:
   $$A^r = \lim_{n\to\infty} A^{r_n}$$
   여기서 $A^{r_n}$은 $r_n$이 유리수일 때 이미 정의된 거듭제곱을 사용하여 계산된다. 이 방법으로 $A^r$는 모든 실수 $r$에 대해 잘 정의된다(well-defined).

| 스칼라 | 행렬 |
|----------|----------|
| 자연수(N) | $2 \times 2 \times 2 = 2^3$ | $A \times A \times A = A^3$ |
| 정수(Z) | $2^{-8} = (2^{-1})^8$ | $A^{-8} = (A^{-1})^8$ |
| 유리수(Q) | $2^{\frac{n}{m}} = (2^{\frac{1}{m}})^n$ | $A^{\frac{n}{m}} = (A^{\frac{1}{m}})^n$ |
| 실수(R) | $2^{1.4}, 2^{1.41}, 2^{1.414}, \ldots \to 2^{\sqrt{2}}$ | $A^{1.4}, A^{1.41}, A^{1.414}, \cdots \to A^{\sqrt{2}}$ |

## 1.3 행렬 지수함수(matrix exponential)

$n \times n$ 행렬 $A$에 대하여 $e^A$는 어떻게 계산해야할까? 일단 간단히 $e^{1.7}$ 값은 어떻게 계산할수 있는가? 일반적으로 테일러 급수를 이용하여 근사적으로 계산가능하다. 지수함수 $e^x$의 테일러 급수(Taylor series)는 다음과 같다.

$e^x = 1 + x + \frac{1}{2!}x^2 + \frac{1}{3!}x^3 + \cdots + \frac{1}{10!}x^{10} + \cdots$

> **물리학과 미분방정식에서의 응용**: 행렬 지수함수는 선형 상미분방정식(ODE) 시스템 $\frac{d\mathbf{x}}{dt} = A\mathbf{x}$의 해를 $\mathbf{x}(t) = e^{At}\mathbf{x}(0)$ 형태로 표현할 수 있게 해준다. 이는 양자역학에서 슈뢰딩거 방정식의 해를 구하거나, 제어 이론에서 시스템의 응답을 분석하는 데 필수적이다. 또한 주목할 점은 행렬 지수함수의 계산이 수치적으로 불안정할 수 있어, 실제 구현에서는 Padé 근사법이나 스케일링과 제곱 방법(scaling and squaring method)과 같은 특수한 알고리즘을 사용한다는 것이다.

이와 유사하게 행렬지수 함수를 테일러 급수를 이용하여 정의할수 있다.

**정의 1.4.** $n \times n$ 행렬 $A$에 대하여 $e^A$는 다음과 같이 정의한다.

$$e^A = I + A + \frac{1}{2!}A^2 + \frac{1}{3!}A^3 + \cdots + \frac{1}{10!}A^{10} + \cdots$$

$e^A b$는 어떻게 계산할수 있을까?

$$e^A b = (I + A + \frac{1}{2!}A^2 + \frac{1}{3!}A^3 + \cdots)b$$

이것은 대략 다음과 같이 근사할 수 있다.

$$e^A b \approx I + Ab + \frac{1}{2!}A^2b + \frac{1}{3!}A^3b + \cdots + \frac{1}{10!}A^{10}b$$

행렬의 거듭제곱 $A^{m+1}$의 계산 비용은 $O(n^3)$이다. 그리고나서 $b$를 곱하여 $A^{m+1}b$를 계산하는것보다 $A(A^m b)$, 즉 행렬과 벡터의 곱으로 반복적으로 계산함으로서 그 계산 비용을 줄일수 있다.

## 1.4 행렬 역행렬 (matrix inversion)

$n \times n$ 정사각행렬 $A$에 대하여 선형 시스템 $Ax = b$가 주어졌을 때, 역행렬 $A^{-1}$이 존재한다면 이 시스템의 해는 다음과 같다.

$x = A^{-1}b$

> **역행렬의 역설**: 컴퓨터 과학과 수치해석에서 흥미로운 패러독스 중 하나는, 선형 시스템 $Ax = b$를 풀기 위해 역행렬 $A^{-1}$을 직접 계산하는 것이 좋은 방법이 아니라는 점이다. 실제로 대부분의 수치 라이브러리는 역행렬을 명시적으로 계산하지 않고 LU 분해, QR 분해, 또는 특이값 분해(SVD)와 같은 방법을 사용한다. 이러한 접근법이 수치적으로 더 안정적이고 계산 효율성도 높다.

$n \times n$ 행렬의 역행렬을 계산하는 과정의 계산 복잡도는 일반적으로 $O(n^3)$인데, 이는 가우스 소거법(Gaussian elimination) 또는 LU 분해와 같은 표준 알고리즘을 사용할 때의 복잡도이다. 대규모 행렬, 특히 수백만 또는 수억 차원에 이르는 행렬을 다룰 때, 이러한 역행렬 계산의 복잡도는 매우 높다. 또한, 행렬이 거의 특이(singular)인 경우, 역행렬을 계산하는 것은 수치적으로 불안정하고, 이로 인해 큰 오류를 발생시킨다.

> **희소 행렬과 대규모 시스템**: 현대 과학 계산에서는 매우 큰 차원의 희소 행렬(sparse matrix)을 다루는 경우가 많다. 이러한 행렬은 대부분의 요소가 0인 특성을 가지며, 일반적인 역행렬 계산 방법을 사용하면 희소성이 손실된다. 따라서 켤레 구배법(Conjugate Gradient Method), GMRES(Generalized Minimal Residual Method)와 같은 반복적 방법(iterative methods)이 선호된다. 이러한 알고리즘은 행렬-벡터 곱만을 사용하여 효율적으로 대규모 시스템을 해결할 수 있다.

역행렬 보조정리(matrix inversion lemma)는 주어진 행렬의 역을 구할 때, 그 행렬에 작은 수정이 가해졌을 때의 역을 효율적으로 계산하는 방법을 제공한다.

**도움정리 1.5.** (역행렬 보조정리, matrix inversion lemma) $A$가 가역적(invertible)인 $d \times d$ 행렬이고, $u$와 $v$가 0이 아닌 $d$-차원의 열 벡터들일 때, $A + uv^T$가 가역적인 동치조건은 $v^T A^{-1}u \neq -1$이다. 이러한 경우에, 역행렬은 다음과 같이 계산된다.

$$(A + uv^T)^{-1} = A^{-1} - \frac{1}{1 + v^T A^{-1}u}A^{-1}uv^T A^{-1}. \tag{1.1}$$

**증명.** 만약 행렬 $A + uv^T$가 가역이면, $A + uv^T$와 $A^{-1}$의 두 가역 행렬의 곱 $(A + uv^T)A^{-1}$도 가역이다. 따라서

$$0 \neq (A + uv^T)A^{-1}u = u + uv^T A^{-1}u = u(1 + v^T A^{-1}u)$$

그러므로 $1 + v^T A^{-1}u \neq 0$.

역으로 $1 + v^T A^{-1}u \neq 0$이면 $A^{-1} - \frac{1}{1+v^T A^{-1}u}A^{-1}uv^T A^{-1}$는 잘 정의된다 (well-defined). 그러면

$$\left(A^{-1} - \frac{1}{1 + v^T A^{-1}u}A^{-1}uv^T A^{-1}\right)(A + uv^T)$$
$$= I + A^{-1}uv^T - \frac{A^{-1}uv^T + A^{-1}uv^T A^{-1}uv^T}{1 + v^T A^{-1}u}$$
$$= I + A^{-1}uv^T - \frac{A^{-1}uv^T(1 + v^T A^{-1}u)}{1 + v^T A^{-1}u}$$
$$= I.$$

그러므로 (1.1)이 성립한다.

**정리 1.6** (Sherman-Morrison-Woodbury Identity) $A$가 가역 가능한 $d \times d$ 행렬이고 $U, V$가 $d \times k$의 0이 아닌 행렬들이며 $k$는 작은 값일 때, 행렬 $A + UV^T$가 가역 가능할 때와 동치조건은 $k \times k$ 행렬 $I + V^T A^{-1}U$이 가역 가능하다이다.

더욱이, 역행렬은 다음과 같이 주어진다.
$$(A + UV^T)^{-1} = A^{-1} - A^{-1}U(I + V^T A^{-1}U)^{-1}V^T A^{-1}$$

## 1.5 행렬 노름 (matrix norm)

행렬 노름도 노름 정의를 그대로 만족한다. 다시 한번 상기시키기 위해서 행렬 노름을 다시 서술하면 다음과 같다.

**정의 1.7.** 행렬 노름은 함수 $\| \cdot \| : \mathbb{R}^{m \times n} \to \mathbb{R}$ 인데 다음의 세 조건을 만족해야 한다.

(i) 모든 $A \in \mathbb{R}^{m \times n}$에 대해 $\|A\| \geq 0$이고, $\|A\| = 0$이면 $A = 0$이다.

(ii) 모든 실수 $k \in \mathbb{R}$와 행렬 $A \in \mathbb{R}^{m \times n}$에 대해 $\|kA\| = |k| \cdot \|A\|$이다.

(iii) 모든 행렬 $A, B \in \mathbb{R}^{m \times n}$에 대해 $\|A + B\| \leq \|A\| + \|B\|$이다.

행렬 노름의 대표적인 예로는 크게 프로베니우스 노름(Frobenius norm)과 작용소 노름(operator norm)이 2가지가 있다.

> **노름의 기하학적 의미**: 행렬 노름은 다차원 공간에서의 '크기'를 측정하는 방법으로 볼 수 있다. 프로베니우스 노름은 행렬의 모든 요소를 고려한 '평균적 크기'를 측정하는 반면, 작용소 노름은 행렬이 가장 크게 영향을 미치는 방향으로의 '최대 확장 효과'를 측정한다. 이러한 서로 다른 노름은 각각 다른 용도에 적합하다: 프로베니우스 노름은 데이터 피팅이나 저차원 근사(low-rank approximation)에서, 작용소 노름은 수치적 안정성이나 수렴 분석에서 주로 사용된다.

### 1.5.1 프로베니우스 노름(Frobenius norm)

**정의 1.8.** $m \times n$ 크기인 두 행렬 $A$와 $B$의 프로베니우스 내적(Frobenius inner product)은 다음과 같이 정의된다.

$\langle A, B \rangle_F = \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij} b_{ij}$

여기서 $a_{ij}$는 $A$의 $(i, j)$번째 원소이고, $b_{ij}$는 $B$의 $(i, j)$ 번째 원소이다.

프로베니우스 내적은 두 행렬의 동일한 위치에 있는 성분들의 곱을 모두 합한 것으로, 이는 벡터 공간 $\mathbb{R}^{mn}$에서의 표준 내적 정의와 일치한다. 프로베니우스 내적 $\langle A, B \rangle_F$는 $\text{Tr}(A^T B)$로 나타낼수 있다.

> **데이터 과학에서의 활용**: 프로베니우스 노름은 기계학습과 데이터 과학에서 널리 사용된다. 특히 PCA(Principal Component Analysis)와 같은 차원 축소 기법이나 SVD(Singular Value Decomposition)를 활용한 저차원 근사에서 중요한 역할을 한다. 또한 행렬 완성(matrix completion) 문제에서 정규화 항으로 사용되어 과적합을 방지하고, 추천 시스템에서 사용자-아이템 상호작용 행렬의 근사 품질을 평가하는 데도 활용된다.

**예제 1.4**
$\mathbb{R}^{2 \times 2}$에서 두 행렬의 프로베니우스 내적(Frobenius inner product)
$\left\langle \begin{bmatrix} 2 & 3 \\ 1 & 5 \end{bmatrix}, \begin{bmatrix} 4 & 1 \\ 2 & 7 \end{bmatrix} \right\rangle_F = 48$
는 정확히 $\mathbb{R}^4$에서 두 백터의 내적
$(2, 3, 1, 5),(4, 1, 2, 7)= 48.$
와 같다.

프로베니우스 내적도 내적의 정의에 나오는 조건들을 모두 만족한다. 두 행렬의 프로베니우스 내적이 0이면 프로베니우스 직교(Frobenius orthogonal)한다라고 말한다.

**정의 1.9.** 프로베니우스 노름(Frobenius norm)은 행렬의 모든 요소의 제곱합의 제곱근으로 정의된다. 행렬 $A$에 대한 프로베니우스 노름은 다음과 같이 정의된다.

$\|A\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}$

여기서 $a_{ij}$는 행렬 $A$의 $i$번째 행과 $j$ 번째 열에 위치한 원소이다.

프로베니우스 노름은 프로베니우스 내적으로 정의된다. 즉, $\|A\|_F = \sqrt{\langle A, A \rangle_F} = \sqrt{\text{Tr}(A^T A)}$가 성립한다. 또한 프로베니우스 노름은 행벡터나 열벡터들의 노름의 제곱의 합으로 나타낼수 있다.

**도움정리 1.10.** $m \times n$ 행렬 $A$에 대하여 다음을 만족한다.

$\|A\|^2_F = \sum_{i=1}^{m} \|a_{i,:}\|^2_2 = \sum_{j=1}^{n} \|a_{:,j}\|^2_2,$

여기서 $a_{i,:}$는 $A$의 $i$번째 행벡터이고 $a_{:,j}$는 $A$의 $j$ 번째 열벡터이다.

프로베니우스 행렬 거리는 두 행렬 간의 차이를 측정하는 방법 중 하나로, 두 행렬의 차이에 대한 프로베니우스 노름을 사용하여 다음과 같이 정의된다.

**정의 1.11.** 두 행렬 $A$와 $B$ 사이의 프로베니우스 거리는 다음과 같다.

$d_F(A, B) = \|A - B\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij} - b_{ij}|^2}$

여기서 $a_{ij}$와 $b_{ij}$는 각각 행렬 $A$와 $B$의 $i$번째 행, $j$ 번째 열에 위치한 원소이다.

**예제 1.5**
계수(rank) 3인 행렬을 계수 1인 rank-one 행렬로 근사한 오차는 다음과 같다.

$\left\|\begin{bmatrix} 2 & 3 & 5 \\ 4 & 5 & 2 \\ 6 & 7 & 7 \end{bmatrix} - \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}\begin{bmatrix} 2 & 3 & 2 \end{bmatrix}\right\|_F = \sqrt{19}$

### 1.5.2 작용소 노름(operator norm)

선형 대수학에서 선형변환은 벡터 공간에서 다른 벡터 공간으로의 매핑을 다루며, 이러한 변환에 의해 벡터의 길이가 얼마나 변하는지를 정량화하는 방법이 중요하게 여겨진다. 작용소 노름은 이러한 변화의 최대 비율을 제공하여 선형변환의 강도를 측정한다. 이는 스펙트럼 노름(spectral norm)이라고도 불린다. 참고로, 작용소 노름은 무한 차원 공간에서의 함수들과 연산자(작용소)들의 성질을 연구하는 분야인 함수해석학(functional analysis)의 발전과 밀접하게 연관되어 있으며, 20세기 동안 많은 연구가 이루어졌다.

> **스펙트럼 해석과의 관계**: 작용소 노름 $\|A\|_{op}$는 행렬 $A^TA$의 가장 큰 고유값의 제곱근과 같다. 즉, $\|A\|_{op} = \sqrt{\lambda_{max}(A^TA)} = \sigma_{max}(A)$로, 이는 $A$의 최대 특이값이다. 이 관계는 선형 시스템의 안정성, 반복 알고리즘의 수렴 특성, 그리고 동적 시스템의 장기적 행동을 분석하는 데 핵심적이다. 예를 들어, 반복 방법 $x_{k+1} = Ax_k + b$의 수렴 속도는 $\|A\|_{op} < 1$일 때 보장되며, 그 수렴 속도는 $\|A\|_{op}$에 의존한다.

**정의 1.12.** 크기가 $m \times n$인 행렬 $A$의 작용소 노름은 다음과 같이 정의된다.

$\|A\|_{op} = \sup_{\|x\| \neq 0} \frac{\|Ax\|}{\|x\|} = \sup_{\|x\|=1} \|Ax\|$

여기서 $\|x\|$는 $\mathbb{R}^n$ 공간에서의 벡터 $x$의 노름이고, $\|Ax\|$는 $\mathbb{R}^m$ 공간에서 $Ax$의 노름이다. 여기서 $\sup$는 주어진 조건을 만족시키는 모든 값들의 상한(supremum)을 의미한다.

(주의사항): 작용소 노름을 $\|A\|_{op}$ 대신 $\|A\|_2$ 또는 $\|A\|$로 표기하기도 한다. 본 교재에서는 혼란을 막기위해 항상 $op$를 붙여서 작용소 노름을 표기한다.

**예제 1.6**
주어진 행렬
$A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$
에 대해 작용소 노름을 구해보자. 작용소 노름의 정의에 의해 다음을 계산하여야한다.
$\|A\|_{op} = \max_{\|x\|=1} \|Ax\|$

여기서 $x = (x_1, x_2)$는 단위 벡터이며, 이 문제는 최적화 문제로 다음과 같이 표현가능하다.

$\text{maximize } \sqrt{(1x_1 + 2x_2)^2 + (3x_1 + 4x_2)^2}$
$\text{subject to } x^2_1 + x^2_2 = 1$

파이썬 'scipy.optimize'의 'minimize' 함수를 사용하여 최적화 문제의 근사해를 구하면 다음과 같은 결과를 얻는다.

$\|A\|_{op} \approx 5.4650$

그 때 최적해 $x$는 다음과 같다.

$x \approx \begin{bmatrix} 0.5761 \\ 0.8174 \end{bmatrix}$

일반적으로 정확한 값을 구할수 없지만 이 예시 경우는 $x = (\cos \theta, \sin \theta)$두고 풀면 정확한 값을 구할 수 있다.

**정의 1.13.** 행렬 노름 $\| \cdot \|$이 곱셈에 대하여 준보존적(sub-multiplicative)이라 함은, 임의의 두 행렬 $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}$에 대하여

$\|AB\| \leq \|A\| \|B\|.$

을 만족할 때를 말한다. 몇몇 교재에서는 준보존적 행렬 노름만을 행렬 노름으로 간주하기도 한다.

**정리 1.14**
두 벡터 $x = (x_1, \cdots, x_n), y = (y_1, \cdots, y_n) \in \mathbb{R}^n$ 에 대하여 다음이 성립한다.

$|\langle x, y \rangle|^2 \leq \langle x, x \rangle \cdot \langle y, y \rangle$

이 부등식을 코시-슈바르츠 부등식(Cauchy-Schwarz inequality)이라고 부른다.

**정리 1.15**
행렬 프로베니우스 노름은 곱셈에 대하여 준보존적이다. 즉, 임의의 두 행렬 $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}$에 대하여

$\|AB\|_F \leq \|A\|_F\|B\|_F$

이 성립한다.

**증명.** $C = [c_{ij}] = AB \in \mathbb{R}^{m \times p}$라 하자. 정의에 의하면,

$\|C\|^2_F = \sum_{i=1}^{m} \sum_{j=1}^{p} c^2_{ij} = \sum_{i=1}^{m} \sum_{j=1}^{p} (A_{i,:}B_{:,j})^2$

코시-슈바르츠 부등식을 사용하면 모든 $i, j$ 에 대하여 다음이 성립한다.

$\langle A^T_{i,:}, B_{:,j} \rangle \leq \|A_{i,:}\|_F\|B_{:,j}\|_F$

그러므로 다음이 성립한다.

$\|C\|^2_F \leq \sum_{i=1}^{m} \sum_{j=1}^{p} \|A_{i,:}\|^2 \|B_{:,j}\|^2 = \left( \sum_{i=1}^{m} \|A_{i,:}\|^2 \right) \left( \sum_{j=1}^{p} \|B_{:,j}\|^2 \right) = \|A\|^2_F \|B\|^2_F$

양변의 제곱근을 취하면 증명이 완료된다.

**정리 1.16**
행렬 작용소 노름은 곱셈에 대하여 준보존적이다. 즉, 임의의 두 행렬 $A \in \mathbb{R}^{m \times n}, B \in \mathbb{R}^{n \times p}$에 대해,

$\|AB\|_{op} \leq \|A\|_{op} \|B\|_{op}.$

**증명.** 임의의 0이 아닌 벡터 $x \in \mathbb{R}^p$를 고려하자. 작용소 노름의 정의에 의해 다음이 성립한다.

$\|ABx\| \leq \|A\|_{op} \cdot \|Bx\| \leq \|A\|_{op} \cdot \|B\|_{op} \cdot \|x\|$

$\|x\|$를 양변에 나누면 다음과 같다.

$\frac{\|ABx\|}{\|x\|} \leq \|A\|_{op} \cdot \|B\|_{op}$

모든 0이 아닌 $x$에 대해 왼쪽 항의 최댓값을 취하면,

$\|AB\|_{op} \leq \|A\|_{op} \|B\|_{op}$

이 된다.

마지막으로 노름과 관련된 주요 부등식 몇 가지를 알아보자.

**정리 1.17**
두 벡터 $x = (x_1, \cdots, x_n), y = (y_1, \cdots, y_n) \in \mathbb{R}^n$ 에 대하여 다음이 성립한다.

$\|x \circ y\|_1 \leq \|x\|_p\|y\|_q,$

여기서 $p, q$는 $1/p + 1/q = 1$를 만족하는 1이상의 실수이고 $\circ$는 하다마드 곱셈(Hadamard product)을 뜻한다, 즉 원소별 곱셈이다. 이 부등식을 횔더 부등식(Hölder's inequality)이라고 부른다.

**정리 1.18**
두 벡터 $x = (x_1, \cdots, x_n), y = (y_1, \cdots, y_n) \in \mathbb{R}^n$ 에 대하여 다음이 성립한다.

$\|x + y\|_p \leq \|x\|_p + \|y\|_p$

여기서 $p, q$는 $1/p + 1/q = 1$를 만족하는 1이상의 실수이다. 이 부등식을 민코프스키 부등식(Minkowski inequality)이라고 부른다.

## 1.6 행렬의 조건수 (condition number)

조건수(condition number)는 행렬이나 함수의 해에 대한 입력 데이터의 작은 변화에 대한 민감도를 나타내는 척도이다. 조건수가 큰 값이면 작은 입력의 변화가 결과 큰 영향을 미치게 된다. 가령 $2 \times 2$ 행렬 $A$를 고려하자.

$A = \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$

여기서 $A$는 역행렬이 존재하지 않는다. $A$의 대각성분에 아주 미세한 값 $\epsilon = 10^{-8}$을 더하여 다음과 같이 $A_\epsilon$을 만든다.

$A_\epsilon = \begin{bmatrix} 1 + 10^{-8} & 1 \\ 1 & 1 + 10^{-8} \end{bmatrix}.$

이 섭동(perturbation) 행렬은 역행렬이 존재하므로 사용이 용이하다. 일반적으로 $\epsilon = 10^{-8}$ 미세한 값이므로 무시해도 문제가 없을 것처럼 보이나 다음을 보자.

$A^{-1}_\epsilon \approx 10^8/2 \begin{bmatrix} 1 + 10^{-8}/2 & -1 + 10^{-8}/2 \\ -1 + 10^{-8}/2 & 1 + 10^{-8}/2 \end{bmatrix}$

원래 행렬에서는 아주 작은 값의 변화이지만, 역행렬에서는 이 변화가 매우 크게 증폭되어 큰 차이를 초래할 수 있다.

**정의 1.19.** $n \times n$ 정사각 행렬 $A$에 대한 조건수 $\kappa(A)$는 다음과 같이 정의된다.

$\kappa(A) = \|A\|_{op}\|A^{-1}\|_{op}$

여기서 $\| \cdot \|_{op}$는 행렬의 작용소 노름을 나타낸다. 조건수가 클때 행렬은 나쁜 조건(ill-coditioned)이라고 말한다.

행렬 $A$의 조건수는 행렬 $A$의 특이값(singular value)으로 표현가능하다.

**기초정리 1.20.** 주어진 행렬 $A$에 대하여 다음이 성립한다.

$\kappa(A) = \frac{\sigma_{max}(A)}{\sigma_{min}(A)}$

여기서 $\sigma_{max}(A)$는 $A$의 최대 특이값, $\sigma_{min}(A)$는 $A$의 최소 특이값이다.

## 1.7 연습문제

**문제 1.1**
만약 정사각형 행렬 $A$가 $A = -A^T$를 만족한다면, 임의의 열 벡터 $x$에 대해 $x^T Ax = 0$임을 보여라.

**문제 1.2**
$n \times n$ 행렬 $A$가 어떤 $n \times d$ 행렬 $D$에 대해 $A = DD^T$를 만족한다고 가정하자. 임의의 열 벡터 $x \in \mathbb{R}^n$에 대하여 $x^T Ax \geq 0$임을 보여라.

**문제 1.3**
임의의 $n \times n$ 정사각행렬 $A$에 대해서 다음이 성립함을 보여라.
$(I + A)^{-1} = I - (I + A)^{-1}A$
여기서, $I$는 $n \times n$ 단위행렬(identity matrix)이다.

**문제 1.4**
다음과 같이 $2 \times 2$ 행렬 $A$를 고려하자.
$A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$
복소수 값을 허용하더라도 $A^{\frac{1}{2}}$가 존재하지 않음을 보여라.

**문제 1.5**
충분히 작은 $\epsilon$에 대해서 $d \times d$ 행렬 $A_\epsilon = A + \epsilon B$를 고려하자. 다음 근사식을 만족함을 보여라.
$A^{-1}_\epsilon \approx A^{-1} - \epsilon A^{-1}BA^{-1}$

**문제 1.6**
$A$와 $B$가 $n \times d$ 행렬이라 하자. 이들을 열 기준으로 $A = [A_1, A_2]$와 $B = [B_1, B_2]$로 분할할 수 있다. 여기서 $A_1$과 $B_1$은 각각 $A$와 $B$의 처음 $k$개 열을 같은 순서로 포함하는 $n \times k$ 행렬이다. $A_2$와 $B_2$는 나머지 열들을 포함한다. 행렬 곱 $AB^T$가 다음과 같이 표현될 수 있음을 보여라.
$AB^T = A_1B^T_1 + A_2B^T_2$